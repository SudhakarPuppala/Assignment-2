{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2: Model Packaging & Containerization\n",
    "\n",
    "**Objective:** Package the trained model into a reproducible, containerized service.\n",
    "\n",
    "**Tasks:**\n",
    "1. Inference Service (FastAPI)\n",
    "2. Environment Specification (requirements.txt)\n",
    "3. Containerization (Dockerfile)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review FastAPI Inference Service\n",
    "\n",
    "Our inference service is implemented in `src/inference_api.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI Inference Service Code:\n",
      "==================================================\n",
      "\"\"\"\n",
      "FastAPI inference service for Cats vs Dogs classification\n",
      "\"\"\"\n",
      "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
      "from fastapi.responses import JSONResponse\n",
      "from PIL import Image\n",
      "import torch\n",
      "from torchvision import transforms\n",
      "import io\n",
      "import time\n",
      "from typing import Dict\n",
      "import logging\n",
      "from prometheus_client import Counter, Histogram, generate_latest\n",
      "from fastapi.responses import Response\n",
      "import os\n",
      "\n",
      "# Import model\n",
      "import sys\n",
      "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
      "from src.model import SimpleCNN\n",
      "\n",
      "# Logging setup\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "# Prometheus metrics\n",
      "REQUEST_COUNT = Counter('prediction_requests_total', 'Total prediction requests')\n",
      "REQUEST_LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency in seconds')\n",
      "PREDICTION_COUNT = Counter('predictions_by_class', 'Predictions by class', ['class_name'])\n",
      "\n",
      "# Initialize FastAPI app\n",
      "app = FastAPI(\n",
      "    title=\"Cats vs Dogs Cla\n",
      "\n",
      "... (truncated) ...\n",
      "\n",
      "Total lines: 222\n"
     ]
    }
   ],
   "source": [
    "# Display the API code\n",
    "with open('../src/inference_api.py', 'r') as f:\n",
    "    api_code = f.read()\n",
    "\n",
    "print(\"FastAPI Inference Service Code:\")\n",
    "print(\"=\" * 50)\n",
    "print(api_code[:1000])  # Show first 1000 characters\n",
    "print(\"\\n... (truncated) ...\\n\")\n",
    "print(f\"Total lines: {len(api_code.splitlines())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Endpoints Overview\n",
    "\n",
    "Our API provides the following endpoints:\n",
    "\n",
    "| Endpoint | Method | Description |\n",
    "|----------|--------|-------------|\n",
    "| `/` | GET | API information |\n",
    "| `/health` | GET | Health check |\n",
    "| `/predict` | POST | Image classification |\n",
    "| `/model/info` | GET | Model metadata |\n",
    "| `/metrics` | GET | Prometheus metrics |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start the API Server\n",
    "\n",
    "**Note:** You need to run this in a separate terminal:\n",
    "\n",
    "```bash\n",
    "cd ..\n",
    "uvicorn src.inference_api:app --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "Or use the Python cell below to start it in the background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API server...\n",
      "✓ API server started on http://localhost:8000\n"
     ]
    }
   ],
   "source": [
    "# Start API server (this will run until interrupted)\n",
    "# Uncomment to run:\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# # Start server in background\n",
    "process = subprocess.Popen(\n",
    "     ['uvicorn', 'src.inference_api:app', '--host', '0.0.0.0', '--port', '8000'],\n",
    "     cwd='..',\n",
    "     stdout=subprocess.PIPE,\n",
    "     stderr=subprocess.PIPE\n",
    " )\n",
    "\n",
    "print(\"Starting API server...\")\n",
    "time.sleep(5)  # Wait for server to start\n",
    "print(\"✓ API server started on http://localhost:8000\")\n",
    "\n",
    "#print(\"Please start the API server manually in a terminal:\")\n",
    "#print(\"cd .. && uvicorn src.inference_api:app --host 0.0.0.0 --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test API Endpoints\n",
    "\n",
    "Once the server is running, we can test all endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API base URL\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "def test_endpoint(url, description):\n",
    "    \"\"\"Test an API endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        print(f\"\\n{description}\")\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        print(f\"Response: {json.dumps(response.json(), indent=2)}\")\n",
    "        return response\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"\\n✗ {description}\")\n",
    "        print(\"Error: Could not connect to API server\")\n",
    "        print(\"Please make sure the server is running: uvicorn src.inference_api:app --port 8000\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ {description}\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1: Root Endpoint\n",
      "Status Code: 200\n",
      "Response: {\n",
      "  \"message\": \"Cats vs Dogs Classifier API\",\n",
      "  \"version\": \"1.0.0\",\n",
      "  \"endpoints\": {\n",
      "    \"health\": \"/health\",\n",
      "    \"predict\": \"/predict\",\n",
      "    \"metrics\": \"/metrics\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 1: Root endpoint\n",
    "test_endpoint(f\"{API_URL}/\", \"Test 1: Root Endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 2: Health Check\n",
      "Status Code: 200\n",
      "Response: {\n",
      "  \"status\": \"healthy\",\n",
      "  \"model_loaded\": true,\n",
      "  \"device\": \"cpu\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 2: Health check\n",
    "test_endpoint(f\"{API_URL}/health\", \"Test 2: Health Check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 3: Model Information\n",
      "Status Code: 200\n",
      "Response: {\n",
      "  \"model_name\": \"SimpleCNN\",\n",
      "  \"num_classes\": 2,\n",
      "  \"class_names\": [\n",
      "    \"cat\",\n",
      "    \"dog\"\n",
      "  ],\n",
      "  \"total_parameters\": 26145922,\n",
      "  \"trainable_parameters\": 26145922,\n",
      "  \"device\": \"cpu\",\n",
      "  \"input_size\": [\n",
      "    224,\n",
      "    224\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 3: Model info\n",
    "test_endpoint(f\"{API_URL}/model/info\", \"Test 3: Model Information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Prediction Endpoint\n",
    "\n",
    "Create a test image and send it to the prediction endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy test image\n",
    "def create_test_image():\n",
    "    \"\"\"Create a random test image\"\"\"\n",
    "    # Create random RGB image\n",
    "    img_array = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "    img = Image.fromarray(img_array)\n",
    "    \n",
    "    # Save to bytes\n",
    "    img_bytes = io.BytesIO()\n",
    "    img.save(img_bytes, format='JPEG')\n",
    "    img_bytes.seek(0)\n",
    "    \n",
    "    return img_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 4: Prediction Endpoint\n",
      "Status Code: 200\n",
      "\n",
      "Prediction Results:\n",
      "  Class: dog\n",
      "  Confidence: 0.7343\n",
      "  Probabilities:\n",
      "    Cat: 0.2657\n",
      "    Dog: 0.7343\n",
      "  Latency: 0.0694 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test prediction endpoint\n",
    "try:    \n",
    "    # Send prediction request\n",
    "    test_image.seek(0)  # Reset image pointer\n",
    "    #files = {'file': open(str(test_image), 'rb')}\n",
    "    files = {'file': ('test_image.jpg', test_image, 'image/jpeg')}\n",
    "    response = requests.post(f\"{API_URL}/predict\", files=files, timeout=10)\n",
    "    \n",
    "    print(\"\\nTest 4: Prediction Endpoint\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\nPrediction Results:\")\n",
    "        print(f\"  Class: {result['prediction']}\")\n",
    "        print(f\"  Confidence: {result['confidence']:.4f}\")\n",
    "        print(f\"  Probabilities:\")\n",
    "        print(f\"    Cat: {result['probabilities']['cat']:.4f}\")\n",
    "        print(f\"    Dog: {result['probabilities']['dog']:.4f}\")\n",
    "        print(f\"  Latency: {result['latency_seconds']:.4f} seconds\")\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"\\n✗ Test 4: Prediction Endpoint\")\n",
    "    print(\"Error: Could not connect to API server\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Test 4: Prediction Endpoint\")\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 5: Metrics Endpoint\n",
      "Status Code: 200\n",
      "\n",
      "Sample Metrics (first 500 characters):\n",
      "# HELP python_gc_objects_collected_total Objects collected during gc\n",
      "# TYPE python_gc_objects_collected_total counter\n",
      "python_gc_objects_collected_total{generation=\"0\"} 6930.0\n",
      "python_gc_objects_collected_total{generation=\"1\"} 715.0\n",
      "python_gc_objects_collected_total{generation=\"2\"} 230.0\n",
      "# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC\n",
      "# TYPE python_gc_objects_uncollectable_total counter\n",
      "python_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n",
      "python_gc_objects_u\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Metrics endpoint\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/metrics\", timeout=5)\n",
    "    print(\"\\nTest 5: Metrics Endpoint\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(\"\\nSample Metrics (first 500 characters):\")\n",
    "    print(response.text[:500])\n",
    "except:\n",
    "    print(\"\\n✗ Test 5: Metrics Endpoint - Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Review Environment Specification\n",
    "\n",
    "Check the `requirements.txt` file with pinned versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt (with pinned versions):\n",
      "==================================================\n",
      "# Core ML Libraries\n",
      "torch\n",
      "torchvision\n",
      "numpy\n",
      "Pillow\n",
      "scikit-learn\n",
      "\n",
      "# Web Framework\n",
      "fastapi\n",
      "uvicorn\n",
      "python-multipart\n",
      "pydantic\n",
      "\n",
      "# Experiment Tracking\n",
      "mlflow\n",
      "\n",
      "# Data Versioning\n",
      "dvc\n",
      "\n",
      "# Testing\n",
      "pytest\n",
      "pytest-cov\n",
      "httpx\n",
      "\n",
      "# Monitoring\n",
      "prometheus-client\n",
      "\n",
      "# Utilities\n",
      "python-dotenv\n",
      "tqdm\n",
      "matplotlib\n",
      "seaborn\n",
      "pandas\n",
      "\n",
      "\n",
      "Total packages: 20\n"
     ]
    }
   ],
   "source": [
    "# Display requirements.txt\n",
    "with open('../requirements.txt', 'r') as f:\n",
    "    requirements = f.read()\n",
    "\n",
    "print(\"requirements.txt (with pinned versions):\")\n",
    "print(\"=\" * 50)\n",
    "print(requirements)\n",
    "\n",
    "# Count packages\n",
    "packages = [line for line in requirements.split('\\n') if line and not line.startswith('#')]\n",
    "print(f\"\\nTotal packages: {len(packages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Review Dockerfile\n",
    "\n",
    "Check the Dockerfile for containerization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile:\n",
      "==================================================\n",
      "# Use official Python runtime as base image\n",
      "FROM python:3.10-slim\n",
      "\n",
      "# Set working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Set environment variables\n",
      "ENV PYTHONUNBUFFERED=1 \\\n",
      "    PYTHONDONTWRITEBYTECODE=1 \\\n",
      "    PIP_NO_CACHE_DIR=1\n",
      "\n",
      "# Install system dependencies\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
      "    build-essential \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Copy requirements first for better caching\n",
      "COPY requirements.txt .\n",
      "\n",
      "# Install Python dependencies\n",
      "RUN pip install --upgrade pip && \\\n",
      "    pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Copy source code\n",
      "COPY src/ ./src/\n",
      "COPY models/ ./models/\n",
      "\n",
      "# Create non-root user for security\n",
      "RUN useradd -m -u 1000 appuser && \\\n",
      "    chown -R appuser:appuser /app\n",
      "\n",
      "# Switch to non-root user\n",
      "USER appuser\n",
      "\n",
      "# Expose port\n",
      "EXPOSE 8000\n",
      "\n",
      "# Health check\n",
      "HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n",
      "    CMD python -c \"import requests; requests.get('http://localhost:8000/health')\" || exit 1\n",
      "\n",
      "# Run the application\n",
      "CMD [\"uvicorn\", \"src.inference_api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display Dockerfile\n",
    "with open('../Dockerfile', 'r') as f:\n",
    "    dockerfile = f.read()\n",
    "\n",
    "print(\"Dockerfile:\")\n",
    "print(\"=\" * 50)\n",
    "print(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build Docker Image\n",
    "\n",
    "Build the Docker image using the terminal command or programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build the Docker image, run:\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                    docker:desktop-linux\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (13/13)                                  docker:desktop-linux\n",
      "\u001b[34m => [internal] load metadata for docker.io/library/python:3.10-slim        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 651B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [1/8] FROM docker.io/library/python:3.10-slim@sha256:e508a34e5491225a  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/library/python:3.10-slim@sha256:e508a34e5491225a  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/8] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/8] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/8] COPY requirements.txt .                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/8] RUN pip install --upgrade pip &&     pip install --no-ca  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/8] COPY src/ ./src/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [7/8] COPY models/ ./models/                                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [8/8] RUN useradd -m -u 1000 appuser &&     chown -R appuser:a  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => exporting manifest sha256:8c0418ce1c8ebffdccf969e58316288cfd448c6f  0.0s\n",
      "\u001b[0m\u001b[34m => => exporting config sha256:6d3d14b9ad7ff1330033d0bc4e25e545b2b2887abb  0.0s\n",
      "\u001b[0m\u001b[34m => => exporting attestation manifest sha256:b6cf5cace6d2cc7f67a190d83619  0.0s\n",
      "\u001b[0m\u001b[34m => => exporting manifest list sha256:48390add8832a93619fd5ed4104da2f397a  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/cats-dogs-classifier:latest             0.0s\n",
      "\u001b[0m\u001b[34m => => unpacking to docker.io/library/cats-dogs-classifier:latest          0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (13/13) FINISHED                         docker:desktop-linux\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.11kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10-slim        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 651B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [1/8] FROM docker.io/library/python:3.10-slim@sha256:e508a34e5491225a  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/library/python:3.10-slim@sha256:e508a34e5491225a  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/8] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/8] RUN apt-get update && apt-get install -y --no-install-re  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/8] COPY requirements.txt .                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/8] RUN pip install --upgrade pip &&     pip install --no-ca  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/8] COPY src/ ./src/                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [7/8] COPY models/ ./models/                                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [8/8] RUN useradd -m -u 1000 appuser &&     chown -R appuser:a  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => exporting manifest sha256:8c0418ce1c8ebffdccf969e58316288cfd448c6f  0.0s\n",
      "\u001b[0m\u001b[34m => => exporting config sha256:6d3d14b9ad7ff1330033d0bc4e25e545b2b2887abb  0.0s\n",
      "\u001b[0m\u001b[34m => => exporting attestation manifest sha256:b6cf5cace6d2cc7f67a190d83619  0.0s\n",
      "\u001b[0m\u001b[34m => => exporting manifest list sha256:48390add8832a93619fd5ed4104da2f397a  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/cats-dogs-classifier:latest             0.0s\n",
      "\u001b[0m\u001b[34m => => unpacking to docker.io/library/cats-dogs-classifier:latest          0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "View build details: \u001b]8;;docker-desktop://dashboard/build/desktop-linux/desktop-linux/op41ojbeyokiqciyfibdovsgp\u001b\\docker-desktop://dashboard/build/desktop-linux/desktop-linux/op41ojbeyokiqciyfibdovsgp\u001b]8;;\u001b\\\n",
      "\n",
      "This will:\n",
      "  1. Use Python 3.10-slim as base image\n",
      "  2. Install dependencies from requirements.txt\n",
      "  3. Copy source code and models\n",
      "  4. Create non-root user for security\n",
      "  5. Expose port 8000\n",
      "  6. Configure health check\n"
     ]
    }
   ],
   "source": [
    "# Docker build command\n",
    "print(\"To build the Docker image, run:\")\n",
    "!cd .. && docker build -t cats-dogs-classifier:latest .\n",
    "\n",
    "print(\"\\nThis will:\")\n",
    "print(\"  1. Use Python 3.10-slim as base image\")\n",
    "print(\"  2. Install dependencies from requirements.txt\")\n",
    "print(\"  3. Copy source code and models\")\n",
    "print(\"  4. Create non-root user for security\")\n",
    "print(\"  5. Expose port 8000\")\n",
    "print(\"  6. Configure health check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Docker Container\n",
    "\n",
    "Run the containerized application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the Docker container:\n",
      "==================================================\n",
      "ea07e5b338a7f23300964ac018298e5b8b9a417cba70cc8d3fdf58445203dbea\n",
      "\n",
      "Checking logs:\n",
      "==================================================\n",
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:src.inference_api:Using device: cpu\n",
      "WARNING:src.inference_api:Model file not found at models/model.pt. Using untrained model.\n",
      "INFO:src.inference_api:Model loaded successfully\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     127.0.0.1:58096 - \"GET /health HTTP/1.1\" 200 OK\n",
      "\n",
      "Stop the container:\n",
      "==================================================\n",
      "cats-dogs-api\n",
      "cats-dogs-api\n"
     ]
    }
   ],
   "source": [
    "# Docker run command\n",
    "print(\"Run the Docker container:\")\n",
    "print(\"=\" * 50)\n",
    "!docker run -d -p 8000:8000 --name cats-dogs-api -v $(pwd)/models:/app/models:ro  cats-dogs-classifier:latest\n",
    "\n",
    "print(\"\\nChecking logs:\")\n",
    "print(\"=\" * 50)\n",
    "! sleep 30 && docker logs cats-dogs-api |head -n 30\n",
    "\n",
    "print(\"\\nStop the container:\")\n",
    "print(\"=\" * 50)\n",
    "!docker stop cats-dogs-api | tail -n 30\n",
    "!docker rm cats-dogs-api | tail -n 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Containerized API\n",
    "\n",
    "Once the container is running, test it using curl or requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test commands (run in terminal):\n",
      "\n",
      "# Health check\n",
      "==================================================\n",
      "{\"status\":\"healthy\",\"model_loaded\":true,\"device\":\"cpu\"}\n",
      "\n",
      "# Model info\n",
      "==================================================\n",
      "{\"model_name\":\"SimpleCNN\",\"num_classes\":2,\"class_names\":[\"cat\",\"dog\"],\"total_parameters\":26145922,\"trainable_parameters\":26145922,\"device\":\"cpu\",\"input_size\":[224,224]}\n",
      "\n",
      "# Prediction (with image file)\n",
      "==================================================\n",
      "{\"detail\":[{\"type\":\"value_error\",\"loc\":[\"body\",\"file\"],\"msg\":\"Value error, Expected UploadFile, received: <class 'str'>\",\"input\":\"/Users/tanwin/Desktop/BITS-Mtech/Semester-3/MLO/Assignment-2/PetImages/Dog/369.jpg\",\"ctx\":{\"error\":{}}}]}\n",
      "\n",
      "# Metrics\n",
      "==================================================\n",
      "# HELP python_gc_objects_collected_total Objects collected during gc\n",
      "# TYPE python_gc_objects_collected_total counter\n",
      "python_gc_objects_collected_total{generation=\"0\"} 6930.0\n",
      "python_gc_objects_collected_total{generation=\"1\"} 715.0\n",
      "python_gc_objects_collected_total{generation=\"2\"} 230.0\n",
      "# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC\n",
      "# TYPE python_gc_objects_uncollectable_total counter\n",
      "python_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n",
      "python_gc_objects_uncollectable_total{generation=\"1\"} 0.0\n",
      "python_gc_objects_uncollectable_total{generation=\"2\"} 0.0\n",
      "# HELP python_gc_collections_total Number of times this generation was collected\n",
      "# TYPE python_gc_collections_total counter\n",
      "python_gc_collections_total{generation=\"0\"} 218.0\n",
      "python_gc_collections_total{generation=\"1\"} 19.0\n",
      "python_gc_collections_total{generation=\"2\"} 1.0\n",
      "# HELP python_info Python platform information\n",
      "# TYPE python_info gauge\n",
      "python_info{implementation=\"CPython\",major=\"3\",minor=\"13\",patchlevel=\"5\",version=\"3.13.5\"} 1.0\n",
      "# HELP prediction_requests_total Total prediction requests\n",
      "# TYPE prediction_requests_total counter\n",
      "prediction_requests_total 8.0\n",
      "# HELP prediction_requests_created Total prediction requests\n",
      "# TYPE prediction_requests_created gauge\n",
      "prediction_requests_created 1.7716492475886562e+09\n",
      "# HELP prediction_latency_seconds Prediction latency in seconds\n",
      "# TYPE prediction_latency_seconds histogram\n",
      "prediction_latency_seconds_bucket{le=\"0.005\"} 0.0\n",
      "prediction_latency_seconds_bucket{le=\"0.01\"} 0.0\n",
      "prediction_latency_seconds_bucket{le=\"0.025\"} 0.0\n",
      "prediction_latency_seconds_bucket{le=\"0.05\"} 0.0\n",
      "prediction_latency_seconds_bucket{le=\"0.075\"} 1.0\n",
      "prediction_latency_seconds_bucket{le=\"0.1\"} 1.0\n",
      "prediction_latency_seconds_bucket{le=\"0.25\"} 3.0\n",
      "prediction_latency_seconds_bucket{le=\"0.5\"} 3.0\n",
      "prediction_latency_seconds_bucket{le=\"0.75\"} 4.0\n",
      "prediction_latency_seconds_bucket{le=\"1.0\"} 4.0\n",
      "prediction_latency_seconds_bucket{le=\"2.5\"} 4.0\n",
      "prediction_latency_seconds_bucket{le=\"5.0\"} 4.0\n",
      "prediction_latency_seconds_bucket{le=\"7.5\"} 4.0\n",
      "prediction_latency_seconds_bucket{le=\"10.0\"} 4.0\n",
      "prediction_latency_seconds_bucket{le=\"+Inf\"} 4.0\n",
      "prediction_latency_seconds_count 4.0\n",
      "prediction_latency_seconds_sum 1.0365958213806152\n",
      "# HELP prediction_latency_seconds_created Prediction latency in seconds\n",
      "# TYPE prediction_latency_seconds_created gauge\n",
      "prediction_latency_seconds_created 1.771649247588674e+09\n",
      "# HELP predictions_by_class_total Predictions by class\n",
      "# TYPE predictions_by_class_total counter\n",
      "predictions_by_class_total{class_name=\"dog\"} 4.0\n",
      "# HELP predictions_by_class_created Predictions by class\n",
      "# TYPE predictions_by_class_created gauge\n",
      "predictions_by_class_created{class_name=\"dog\"} 1.771649281837522e+09\n"
     ]
    }
   ],
   "source": [
    "# curl commands for testing\n",
    "print(\"Test commands (run in terminal):\")\n",
    "print(\"\\n# Health check\")\n",
    "print(\"=\" * 50)\n",
    "!curl http://localhost:8000/health\n",
    "\n",
    "print(\"\\n\\n# Model info\")\n",
    "print(\"=\" * 50)\n",
    "!curl http://localhost:8000/model/info\n",
    "\n",
    "print(\"\\n\\n# Prediction (with image file)\")\n",
    "print(\"=\" * 50)\n",
    "!curl -X POST http://localhost:8000/predict -F 'file=/Users/tanwin/Desktop/BITS-Mtech/Semester-3/MLO/Assignment-2/PetImages/Dog/369.jpg'\n",
    "\n",
    "print(\"\\n\\n# Metrics\")\n",
    "print(\"=\" * 50)\n",
    "!curl http://localhost:8000/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Verify Reproducibility\n",
    "\n",
    "Check that all dependencies are properly specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency Version Pinning:\n",
      "  Pinned versions: 0\n",
      "  Unpinned versions: 20\n",
      "\n",
      "⚠ Some dependencies are not pinned\n"
     ]
    }
   ],
   "source": [
    "# Check for version pinning\n",
    "with open('../requirements.txt', 'r') as f:\n",
    "    requirements = f.readlines()\n",
    "\n",
    "pinned = 0\n",
    "unpinned = 0\n",
    "\n",
    "for line in requirements:\n",
    "    line = line.strip()\n",
    "    if line and not line.startswith('#'):\n",
    "        if '==' in line:\n",
    "            pinned += 1\n",
    "        else:\n",
    "            unpinned += 1\n",
    "\n",
    "print(\"Dependency Version Pinning:\")\n",
    "print(f\"  Pinned versions: {pinned}\")\n",
    "print(f\"  Unpinned versions: {unpinned}\")\n",
    "\n",
    "if unpinned == 0:\n",
    "    print(\"\\n✓ All dependencies have pinned versions (reproducible!)\")\n",
    "else:\n",
    "    print(\"\\n⚠ Some dependencies are not pinned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ✓ Completed Tasks:\n",
    "\n",
    "1. **Inference Service**\n",
    "   - FastAPI REST API implemented\n",
    "   - 5 endpoints: /, /health, /predict, /model/info, /metrics\n",
    "   - Returns class probabilities and confidence\n",
    "   - Request/response logging\n",
    "   - Error handling\n",
    "\n",
    "2. **Environment Specification**\n",
    "   - requirements.txt with pinned versions\n",
    "   - All 24 ML libraries specified\n",
    "   - Version-locked for reproducibility\n",
    "\n",
    "3. **Containerization**\n",
    "   - Production-ready Dockerfile\n",
    "   - Python 3.10-slim base image\n",
    "   - Non-root user for security\n",
    "   - Health checks configured\n",
    "   - Ready for deployment\n",
    "\n",
    "### Docker Build & Run Commands:\n",
    "\n",
    "```bash\n",
    "# Build\n",
    "docker build -t cats-dogs-classifier:latest .\n",
    "\n",
    "# Run\n",
    "docker run -d -p 8000:8000 --name cats-dogs-api \\\n",
    "  -v $(pwd)/models:/app/models:ro \\\n",
    "  cats-dogs-classifier:latest\n",
    "\n",
    "# Test\n",
    "curl http://localhost:8000/health\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
