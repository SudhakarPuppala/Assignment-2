{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5: Monitoring, Logs & Final Submission\n",
    "\n",
    "**Objective:** Monitor the deployed model and submit a consolidated package of all artifacts.\n",
    "\n",
    "**Tasks:**\n",
    "1. Basic Monitoring & Logging\n",
    "2. Model Performance Tracking\n",
    "3. Final Submission Checklist\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application Logging:\n",
      "============================================================\n",
      "\n",
      "Logging Configuration:\n",
      "  - Format: Structured JSON\n",
      "  - Level: INFO (configurable)\n",
      "  - Output: stdout (captured by container runtime)\n",
      "\n",
      "Logged Events:\n",
      "  - API startup/shutdown\n",
      "  - Model loading\n",
      "  - Prediction requests\n",
      "  - Request latency\n",
      "  - Errors and exceptions\n",
      "  - Health check calls\n",
      "\n",
      "Example Log Entry:\n",
      "{\n",
      "  \"timestamp\": \"2024-02-10T10:30:45\",\n",
      "  \"level\": \"INFO\",\n",
      "  \"message\": \"Prediction: cat (confidence: 0.92)\",\n",
      "  \"latency_seconds\": 0.045\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Application Logging:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nLogging Configuration:\")\n",
    "print(\"  - Format: Structured JSON\")\n",
    "print(\"  - Level: INFO (configurable)\")\n",
    "print(\"  - Output: stdout (captured by container runtime)\")\n",
    "print(\"\\nLogged Events:\")\n",
    "print(\"  - API startup/shutdown\")\n",
    "print(\"  - Model loading\")\n",
    "print(\"  - Prediction requests\")\n",
    "print(\"  - Request latency\")\n",
    "print(\"  - Errors and exceptions\")\n",
    "print(\"  - Health check calls\")\n",
    "print(\"\\nExample Log Entry:\")\n",
    "log_entry = {\n",
    "    \"timestamp\": \"2024-02-10T10:30:45\",\n",
    "    \"level\": \"INFO\",\n",
    "    \"message\": \"Prediction: cat (confidence: 0.92)\",\n",
    "    \"latency_seconds\": 0.045\n",
    "}\n",
    "print(json.dumps(log_entry, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Logs:\n",
      "============================================================\n",
      "\n",
      "Local (uvicorn):\n",
      "  # Logs appear in terminal\n",
      "\n",
      "Docker:\n",
      "============================================================\n",
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:src.inference_api:Using device: cpu\n",
      "INFO:src.inference_api:Model loaded from models/model.pt\n",
      "INFO:src.inference_api:Model loaded successfully\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     127.0.0.1:42060 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54920 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:55936 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     192.168.65.1:42280 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59118 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:35428 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53940 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     192.168.65.1:23772 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:46460 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:40128 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57384 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:38270 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     172.64.144.78:28594 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:     127.0.0.1:55812 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:48752 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:47912 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:39692 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59144 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57450 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:45548 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59214 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:55144 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:43884 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:44616 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:36472 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52708 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:     127.0.0.1:50952 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:32818 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:60352 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:44840 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "INFO:src.inference_api:Prediction: cat (confidence: 0.6332)\n",
      "\n",
      "Kubernetes:\n",
      "============================================================\n",
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:src.inference_api:Using device: cpu\n",
      "INFO:src.inference_api:Model loaded from models/model.pt\n",
      "INFO:src.inference_api:Model loaded successfully\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     10.244.0.1:34994 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:52414 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:52416 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:47972 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:47986 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:57922 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:57934 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:57942 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:43350 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:43366 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:43380 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53858 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53872 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53880 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53698 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53702 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53716 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:50058 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:50066 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:50078 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:38206 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:38210 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:38218 - \"GET /health HTTP/1.1\" 200 OK\n",
      "Found 2 pods, using pod/cats-dogs-classifier-7b849b88d5-hs44f\n",
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:src.inference_api:Using device: cpu\n",
      "INFO:src.inference_api:Model loaded from models/model.pt\n",
      "INFO:src.inference_api:Model loaded successfully\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     10.244.0.1:34994 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:52414 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:52416 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:47972 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:47986 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:57922 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:57934 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:57942 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:43350 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:43366 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:43380 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53858 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53872 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53880 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53698 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53702 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:53716 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:50058 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:50066 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:50078 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:38206 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:38210 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     10.244.0.1:38218 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "print(\"View Logs:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nLocal (uvicorn):\")\n",
    "print(\"  # Logs appear in terminal\")\n",
    "print(\"\\nDocker:\")\n",
    "print(\"=\" * 60)\n",
    "!docker logs cats-dogs-api | head -n 30\n",
    "\n",
    "print(\"\\nKubernetes:\")\n",
    "print(\"=\" * 60)\n",
    "!kubectl logs $(kubectl get pods -l app=cats-dogs-classifier -o jsonpath='{.items[0].metadata.name}') | head -n 30\n",
    "!kubectl logs  deployment/cats-dogs-classifier | head -n 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prometheus Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prometheus Configuration:\n",
      "============================================================\n",
      "global:\n",
      "  scrape_interval: 15s\n",
      "  evaluation_interval: 15s\n",
      "\n",
      "scrape_configs:\n",
      "  - job_name: 'cats-dogs-classifier'\n",
      "    static_configs:\n",
      "      - targets: ['classifier:8000']\n",
      "    metrics_path: '/metrics'\n",
      "    scrape_interval: 10s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display Prometheus configuration\n",
    "with open('../monitoring/prometheus.yml', 'r') as f:\n",
    "    prom_config = f.read()\n",
    "\n",
    "print(\"Prometheus Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(prom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Metrics:\n",
      "============================================================\n",
      "# HELP python_gc_objects_collected_total Objects collected during gc\n",
      "# TYPE python_gc_objects_collected_total counter\n",
      "python_gc_objects_collected_total{generation=\"0\"} 8311.0\n",
      "python_gc_objects_collected_total{generation=\"1\"} 790.0\n",
      "python_gc_objects_collected_total{generation=\"2\"} 60.0\n",
      "# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC\n",
      "# TYPE python_gc_objects_uncollectable_total counter\n",
      "python_gc_objects_uncollectable_total{generation=\"0\"} 0.0\n",
      "python_gc_objects_uncollectable_total{generation=\"1\"} 0.0\n",
      "python_gc_objects_uncollectable_total{generation=\"2\"} 0.0\n",
      "# HELP python_gc_collections_total Number of times this generation was collected\n",
      "# TYPE python_gc_collections_total counter\n",
      "python_gc_collections_total{generation=\"0\"} 698.0\n",
      "python_gc_collections_total{generation=\"1\"} 63.0\n",
      "python_gc_collections_total{generation=\"2\"} 5.0\n",
      "# HELP python_info Python platform information\n",
      "# TYPE python_info gauge\n",
      "python_info{implementation=\"CPython\",major=\"3\",minor=\"10\",pat\n",
      "\n",
      "... (truncated) ...\n"
     ]
    }
   ],
   "source": [
    "# Fetch metrics from API (if running)\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/metrics\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Current Metrics:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(response.text[:1000])\n",
    "        print(\"\\n... (truncated) ...\")\n",
    "    else:\n",
    "        print(\"Could not fetch metrics\")\n",
    "except:\n",
    "    print(\"API not running. Start with: uvicorn src.inference_api:app\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
